{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENDER RECOGNITION BY VOICE\n",
    "###   SIMAN PARVEEN(521)\n",
    "###   SAWAN AICH(553)\n",
    "###   RAHUL CHATTERJEE(545)\n",
    "####   GUIDED BY KAUSHIK GOSWAMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract:\n",
    "If there is something that differentiates human beings from other animals, it is our ability to use\n",
    "language to speak and communicate with each other.Now, distinguishing a male voice from a female voice\n",
    "seems to be an easy task for a human being.But it becomes difficult when a computer has to identify it\n",
    "whether the voice is of male or female. A human has that natural capability of idenitifying the \n",
    "difference but when it comes to computer, we need to teach it by providing inputs, methodology or \n",
    "different training data and make it learn. In this project, we focus on training the computer which can \n",
    "identify a personâ€™s gender if a voice is given as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRARIES FOR APP\n",
    "from flask import Flask,render_template,url_for,request,redirect\n",
    "from flask_uploads import UploadSet,AUDIO,configure_uploads,extension,UploadConfiguration \n",
    "from werkzeug import secure_filename\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "#LIBRARIES FOR READING DATASET,PREPROCESSING,SPLIT,FEATURE EXTRACTION\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "from scipy.stats import stats\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Csv File Created\n",
    "<a href=\"http://localhost:8888/notebooks/Desktop/Final%20Year%20Project/Dataset_Creation.ipynb\">Click on this For Detailed Knowledge of Feature Extraction And Dataset Creation</a>\n",
    "\n",
    "\n",
    "<a href=\"http://localhost:8888/notebooks/Desktop/Final%20Year%20Project/DatasetCreation2.ipynb\">Click Here to know How we have added more audios to Dataset</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m    12595\n",
       "f     9603\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The csv file is read and stored in 'df' \n",
    "df=pd.read_csv(\"C:\\\\Users\\\\SAGAR\\\\Desktop\\\\Final Year Project\\\\csv_dataset\\\\dataset.csv\")\n",
    "# Counted the number of male and female in the training dataset\n",
    "df['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(df,test_size=0.0)# the whole dataset is actually used for training\n",
    "train.index=range(len(train))# to set the index\n",
    "x_train=train.iloc[:,:-1]#to take all the columns except the 'LABEL' column\n",
    "y_train=train[[\"LABEL\"]]#'LABEL' column is considered a separate dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data using\n",
    "   -- Label Encoder(Female-1,Male-0)\n",
    "   \n",
    "   -- MinMax Scaler(To scale the value between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=preprocessing.LabelEncoder() #this preprocessing technique is used to scale the labels-Female and Male \n",
    "y_train=le.fit_transform(y_train.values.ravel())# the values are being fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=preprocessing.MinMaxScaler()#this preprocessing technique is used to scale the values between 0 and 1 \n",
    "x_train[:]=scaler.fit_transform(x_train.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract the features from the given audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to  extract the following features from the given audio-\n",
    "       -- Dominating Frequency(meanfreq)\n",
    "       -- Min Dominating Frequency(low)\n",
    "       -- Max Dominating Frequency(peak)\n",
    "       -- Standard Deviation(std)\n",
    "       -- Kurtosis(kurt)\n",
    "       -- Skewness(skew)\n",
    "       -- Mode(mode)\n",
    "       -- Median(median)\n",
    "       -- First Quartile(Q25)\n",
    "       -- Third Quartile(Q75)\n",
    "       -- Inter Quartile Range(iqr)\n",
    "       -- Spectral Centroid(sp_cent)\n",
    "       -- Spectral Flatness(sfm)\n",
    "       -- Spectral Roll-off(sp_rolloff)\n",
    "       -- Spectral BandWidth(sp_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(audio):\n",
    "        mean1=[]\n",
    "        skew1=[]\n",
    "        kurtosis1=[]\n",
    "        median1=[]\n",
    "        mode1=[]\n",
    "        std1=[]\n",
    "        low1=[]\n",
    "        peak1=[]\n",
    "        q751=[]\n",
    "        q251=[]\n",
    "        iqr1=[]\n",
    "        sp_cent_1=[]\n",
    "        sfm_1=[]\n",
    "        sp_rolloff_1=[]\n",
    "        sp_bandwidth_1=[]\n",
    "        rate,data=wavfile.read(audio)#Reading the audio wav file \n",
    "        step=int(rate/5)#the size of the window frequency\n",
    "        freqArray2=[]\n",
    "        freqArray1=[]\n",
    "        sp_cent=[]\n",
    "        #Find out the Dominating Frequency Using Fourier\n",
    "        for j in range(1,len(data),step):\n",
    "            fourier=np.fft.fft(data[j:j+step])\n",
    "            freqs=np.fft.fftfreq(len(fourier))#freqs tells you the frequencies associated with the coefficients\n",
    "            imax=np.argmax(np.abs(fourier))#returns the max coeffient\n",
    "            freq=freqs[imax]#returns the frequency assciated with max coefficient\n",
    "            freq_in_hz=abs(freq*rate)\n",
    "            freqArray1.append(freq_in_hz)\n",
    "            x=[f for f in freqArray1 if 20<f<300 and not 46<f<66]#so that only human voice is considered\n",
    "        freqArray2.append(x)\n",
    "        freqArray=[item for sublist in freqArray2 for item in sublist]\n",
    "        \n",
    "        x, sr=librosa.load(audio)#loading the audio\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]#to find the spectral centroid of the audio \n",
    "        flatness=librosa.feature.spectral_flatness(x)#to find the spectral flatness of the audio\n",
    "        rolloff = librosa.feature.spectral_rolloff(x,sr=sr)#to find the spectral rolloff of the audio\n",
    "        bandwidth=librosa.feature.spectral_bandwidth(x,sr=sr)#to find the spectral bandwidth of the audio\n",
    "        \n",
    "        sfm=np.mean(flatness)#to find the mean of all the spectral flatness obtained of an audio\n",
    "        sp_cent=np.mean(spectral_centroids)#to find the mean of all the spectral centroid obtained of an audio\n",
    "        sp_rolloff=np.mean(rolloff)#to find the mean of all the spectral rolloff obtained of an audio\n",
    "        sp_bandwidth=np.mean(bandwidth)#to find the mean of all the spectral bandwidth obtained of an audio\n",
    "        \n",
    "        nobs,minmax,mean,variance,skew,kurtosis=stats.describe(freqArray)#to find out the dominating frquency,skewness and kurtosis of the audio\n",
    "        mode=stats.mode(freqArray).mode[0]#to find out the mode of the frequencies of the audio\n",
    "        std=np.std(freqArray)#to find out the standard deviation \n",
    "        low,peak=minmax#to find out the min and max dominating frequency\n",
    "        q75,q25=np.percentile(freqArray, [75 ,25])#to find out first quartile and second quartile\n",
    "        iqr=q75-q25#to find out the interquartile range\n",
    "        median=np.median(freqArray)#to find out the median\n",
    "\n",
    "        #Here the array for each column in the dataset is created by appending the value obtained for above\n",
    "        #for the given audio\n",
    "        mean1.append(mean/1000)\n",
    "        skew1.append(skew)\n",
    "        kurtosis1.append(kurtosis)\n",
    "        median1.append(median/1000)\n",
    "        mode1.append(mode/1000)\n",
    "        std1.append(std/1000)\n",
    "        low1.append(low/1000)\n",
    "        peak1.append(peak/1000)\n",
    "        q751.append(q75/1000)\n",
    "        q251.append(q25/1000)\n",
    "        iqr1.append(iqr/1000)\n",
    "        sp_cent_1.append(sp_cent)\n",
    "        sfm_1.append(sfm)\n",
    "        sp_rolloff_1.append(sp_rolloff)\n",
    "        sp_bandwidth_1.append(sp_bandwidth)\n",
    "        \n",
    "        d=({'meanfreq':mean1,'sd':std1,'median':median1,'Q25':q251,'Q75':q751,'iqr':iqr1,'skew':skew1,'kurt':kurtosis1,'mode':mode1,'low':low1,'peak':peak1,'centroid':sp_cent_1,'flatness':sfm_1,'spectral rolloff':sp_rolloff_1,'spectral bandwidth':sp_bandwidth_1})\n",
    "        df_audio=pd.DataFrame(data=d)\n",
    "        return df_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Predict the gender of the Person speaking in the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x_test, clf_object):\n",
    "    LABEL=['F','M']#the labels(F-Female,M-Male)\n",
    "    pred=[] \n",
    "    y_pred = clf_object.predict(x_test) #Prediction\n",
    "    y=int(y_pred)\n",
    "    pred.append(LABEL[y])   \n",
    "    return pred #returns either the gender is 'F' or 'M'(M-Male,F-Female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Ensemble Model\n",
    "### Ensemble Model consists of the following three models-\n",
    "       -- Gradient  Boosting\n",
    "       -- Support Vector Machine\n",
    "       -- Random Forest\n",
    "#### To know how we have created and saved the model <a href=\"http://localhost:8888/notebooks/Desktop/Final%20Year%20Project/model_creation.ipynb\">Click Here</a>       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Model Is Ready\n"
     ]
    }
   ],
   "source": [
    "model=pickle.load(open('ensemble_model1.sav','rb'))  #loading the ensemble model\n",
    "print(\"\\n The Model Is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLASK APP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the GUI is created using flask(using python) in Back END\n",
    "### The Front End is created using HTML and CSS\n",
    "### Details Present Here:\n",
    "    clear_all_Uploadedfile()-Function to delete all the audio files present in the back end folder.\n",
    "    allow_audio()-Function to specify which type of audio file is allowed here(.WAV).\n",
    "    The following Functions specfies what the functions returns:\n",
    "        index()-Function to open the Home page of the app.\n",
    "        predict()-Function to play the audio.\n",
    "        gender()-Function to display the gender of the person speaking in the audio.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Mar/2020 13:10:45] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:10:46] \"GET /static/style1.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:10:46] \"GET /static/_87558504_87558503.jpg HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted SantoshSIR.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n",
      "Audio File <FileStorage: 'Pritam.wav' ('audio/wav')> Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:11:06] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:11:07] \"GET /static/audio/Pritam.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Pritam.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:11:37] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:11:39] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Pritam.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:11:55] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:11:55] \"GET /static/audio/mamata2020_02_29_18_29_03.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File <FileStorage: 'mamata2020_02_29_18_29_03.wav' ('audio/wav')> Saved\n",
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\mamata2020_02_29_18_29_03.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:03] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:04] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted mamata2020_02_29_18_29_03.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:13] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:12:13] \"GET /static/audio/Sohini.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File <FileStorage: 'Sohini.wav' ('audio/wav')> Saved\n",
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Sohini.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:21] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:23] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Sohini.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:31] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:12:31] \"GET /static/audio/Meghasree.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File <FileStorage: 'Meghasree.wav' ('audio/wav')> Saved\n",
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Meghasree.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:38] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:12:47] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Meghasree.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n",
      "Audio File <FileStorage: 'Oishi.wav' ('audio/wav')> Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:47] \"GET /static/audio/Oishi.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Oishi.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:55] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:12:58] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Oishi.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:36] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:13:36] \"GET /static/audio/Manjistha.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File <FileStorage: 'Manjistha.wav' ('audio/wav')> Saved\n",
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Manjistha.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:42] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:43] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Manjistha.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:49] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:13:49] \"GET /static/audio/Oishi.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File <FileStorage: 'Oishi.wav' ('audio/wav')> Saved\n",
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Oishi.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:52] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:13:56] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Mar/2020 13:14:02] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted Oishi.wav \n",
      "\n",
      "All Previous Uploaded Audios Deleted\n",
      "Audio File <FileStorage: 'Oishi.wav' ('audio/wav')> Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:14:02] \"GET /static/audio/Oishi.wav HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Audio file: C:\\Users\\SAGAR\\Desktop\\Final Year Project\\static\\audio\\Oishi.wav \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:14:04] \"POST /gender HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Mar/2020 13:14:06] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#Here we delete the previous uploaded files because while getting the audio from the audio folder\n",
    "#in the static folder,always the first audio is considered.\n",
    "#So to overcome this problem,We are deleting the previous uploaded files\n",
    "\n",
    "def clear_all_Uploadedfile():\n",
    "    path_to_upld_folder=os.path.join(os.getcwd(),'static/audio')#to get the path of the folder where the audios are saved\n",
    "    list_of_auds=os.listdir(path_to_upld_folder)#to get the audios present\n",
    "    for aud in list_of_auds:\n",
    "        path_to_aud=os.path.join(path_to_upld_folder,aud)\n",
    "        os.remove(path_to_aud)#delete the existing audios\n",
    "        print(f\"\\nDeleted {aud} \\n\")#print the audio name deleted\n",
    "    print(\"All Previous Uploaded Audios Deleted\")\n",
    "\n",
    "  \n",
    "app=Flask(__name__)#the app is initialised\n",
    "app.config['AUDIO_EXTENSION']=['wav']#to specify the type of audio accepted(to use later in allow_audio()function)\n",
    "#run_with_ngrok(app)#so that anyone can test the GUI if the sever is on\n",
    "\n",
    "#UploadConfiguration(os.path.join(os.getcwd(),'static/audio'),\\\n",
    "                    #allow=app.config['AUDIO_EXTENSION'])\n",
    "\n",
    "#We have allow_audio() function to check whether the user uploads only .wav audio file\n",
    "#We have considered only .wav audio file because .wav audio file contains almost all the features of the \n",
    "#audio. Conversion to .mp3 to .wav is not considered here because the .mp3 type does not contain all the \n",
    "#audio features as .wav.\n",
    "\n",
    "def allow_audio(filename):\n",
    "    if not '.' in filename:#if no extension is present\n",
    "        return False\n",
    "    ext=filename.rsplit('.',1)[1]#splitting the given url on the basis of '.' and getting the type of file uploaded\n",
    "    if ext.lower() in app.config['AUDIO_EXTENSION']:\n",
    "        return True # if we have wav type return that its acceptable\n",
    "    else:\n",
    "        return False # else not acceptable\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')  # the home page is returned\n",
    "\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    if request.method==\"POST\":\n",
    "        clear_all_Uploadedfile()#delete all the previous uploaded audios\n",
    "        if request.files and 'audio' in request.files:#if there is any file in the uploaded region\n",
    "            audio_file = request.files['audio']#audio_file is initialized with the name of the audio file\n",
    "            if not allow_audio(audio_file.filename):#checks whether audio file is not of .wav type\n",
    "                print(\"The audio extension is not allowed\")\n",
    "                return redirect('/')#redirect to the home page(index() function)\n",
    "            \n",
    "            audio_file.save(os.path.join(os.getcwd(),'static/audio',\\\n",
    "                                             secure_filename(audio_file.filename)))#if the type is of wav type, \n",
    "                                                                                #save the file in the folder \n",
    "            print(f'Audio File {audio_file} Saved')\n",
    "            return render_template('predict.html',audio=audio_file.filename)#after the file is saved,\n",
    "                            #the file is post at the predict.html so that we can play the audio\n",
    "\n",
    "\n",
    "@app.route('/gender',methods=['POST','GET'])\n",
    "def gender():\n",
    "    if request.method=='POST':#we have used POST here ,to get the audio file from predict.html\n",
    "        audio_file=request.form['audio']\n",
    "        path=os.path.join(os.getcwd(),'static\\\\audio',audio_file)# from the current directory  \n",
    "                                #goes to static folder and then audio folder and get the whole path\n",
    "        print(f\"\\n The Audio file: {path} \")\n",
    "        df_audio=get_features(path)#the audio path is send to the get_features()function for feature\n",
    "                                #extraction\n",
    "        df_audio[:]=scaler.transform(df_audio)#scaled according to the training dataset\n",
    "        df_test_pred=prediction(df_audio,model)#predict the gender(calls the function prediction())\n",
    "        if df_test_pred==['F']:\n",
    "            df_test='Female'#if person speaking is Female\n",
    "        else:\n",
    "            df_test='Male'#if person speaking is Male\n",
    "        print(\"Result Out\")    \n",
    "        return render_template('gender.html',prediction_text=\"In the given Audio,the person speaking is {}\".format(df_test))\n",
    "                                      #returns the predict.html and the final result\n",
    "        \n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run()#runs the flask app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "### After the completion of the project, we came up with the following conclusions:\n",
    "\t--The accuracy of the machine learning models differs when the dataset being used \n",
    "    for training is changed.\n",
    "\t--If more acoustic features or properties of the input voice is extracted,more \n",
    "    accurate is the result.\n",
    "    --Deep female voice or shrill male voice may sometimes lead to wrong prediction.\n",
    "\t--Better accuracy is obtained when the ratio of male and female voices of the \n",
    "    trained dataset is nearly 1:1.\n",
    "\t--There are major scopes of further betterment of this project, such as separation\n",
    "    of various audios in the foreground and background of an input audio file,and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
